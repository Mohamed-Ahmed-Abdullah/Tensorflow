{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Hidden Layer Networks (NNs)\n",
    "\n",
    "Just proof of concept without a goal \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from pandas import DataFrame as DF, Series "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Toy Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# 2500 is rows and 3 is columns \n",
    "XTrain = np.random.normal(0,5,(2500,3))\n",
    "XTest  = np.random.normal(0,5,(2500,3))\n",
    "\n",
    "def f(x):\n",
    "    y = (x[:,1] + 2*x[:,2] - x[:,0]#linear with no interactions \n",
    "         + x[:,0]*x[:,1]+ x[:,0]* x[:,1]* x[:,2] #linear with interactions \n",
    "         # + x[:,1]*(x[:,2]**2) #quadratic with interactions \n",
    "    )\n",
    "    return y.reshape(-1,1)\n",
    "\n",
    "YTrain = f(XTrain)\n",
    "YTest  = f(XTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss:116.055\n",
      "Epoch 10, loss:109.820\n",
      "Epoch 20, loss:95.011\n",
      "Epoch 30, loss:84.769\n",
      "Epoch 40, loss:76.019\n",
      "Epoch 50, loss:68.763\n",
      "Epoch 60, loss:62.959\n",
      "Epoch 70, loss:57.518\n",
      "Epoch 80, loss:52.697\n",
      "Epoch 90, loss:49.033\n",
      "Epoch 100, loss:45.843\n",
      "Epoch 110, loss:43.494\n",
      "Epoch 120, loss:41.557\n",
      "Epoch 130, loss:39.778\n",
      "Epoch 140, loss:37.830\n",
      "Epoch 150, loss:37.369\n",
      "Epoch 160, loss:35.959\n",
      "Epoch 170, loss:33.900\n",
      "Epoch 180, loss:32.447\n",
      "Epoch 190, loss:31.258\n",
      "Epoch 200, loss:30.108\n",
      "Epoch 210, loss:29.118\n",
      "Epoch 220, loss:27.965\n",
      "Epoch 230, loss:26.985\n",
      "Epoch 240, loss:26.137\n",
      "Epoch 250, loss:25.163\n",
      "Epoch 260, loss:24.532\n",
      "Epoch 270, loss:24.724\n",
      "Epoch 280, loss:24.231\n",
      "Epoch 290, loss:23.827\n",
      "Epoch 300, loss:23.591\n",
      "----------\n",
      "Test Loss: 38.331\n"
     ]
    }
   ],
   "source": [
    "#hyper parameters \n",
    "learning_rate=0.01\n",
    "n_input = 3 #number of observation (features) also it's the number of input nodes \n",
    "n_nodes_1 = 32 #2 nodes in the hidden layer\n",
    "n_nodes_2 = 32 #2 nodes in the hidden layer\n",
    "n_output = 1 #1 nide in the output layer \n",
    "batch_size = 100\n",
    "n_epochs = 300 \n",
    "\n",
    "#None number of observation (features) should be flexible \n",
    "#it allows us to pass the batch size later on\n",
    "X = tf.placeholder('float32',[None, n_input])\n",
    "Y = tf.placeholder('float32',[None, n_output])\n",
    "\n",
    "def create_weights(shape):\n",
    "    #create layer weights as variables \n",
    "    #use random_normal initializer and keep the values small by default \n",
    "    #the standard diviation to 0.1 (default is 1.0)\n",
    "    initializer=tf.random_normal(shape,stddev=0.1)\n",
    "    W = tf.Variable(initializer)\n",
    "    return W\n",
    "\n",
    "def create_biases(shape):\n",
    "    #create biase parameter as variables \n",
    "    #should be rank 1 tensor with shape same as n_nodes of layers\n",
    "    initializer=tf.random_normal(shape)\n",
    "    B = tf.Variable(initializer)\n",
    "    return B\n",
    "\n",
    "# assign the wieghts we want for this model \n",
    "weights = {'w1': create_weights([n_input, n_nodes_1]) ,#first and only hidder layer weights\n",
    "           'w2': create_weights([n_nodes_1, n_nodes_2]) ,\n",
    "           'w_out': create_weights([n_nodes_2, n_output]) }#output layer weights \n",
    "\n",
    "# assign biases \n",
    "biases = {'b1':create_biases([n_nodes_1]), #because the bias is for the hidden layers thats why we used n_nodes_l\n",
    "          'b2':create_biases([n_nodes_2]),\n",
    "          'b_out':create_biases([n_output])}\n",
    "\n",
    "# define the network forward propagation \n",
    "\n",
    "# hidden layer 1\n",
    "z1 = tf.add(tf.matmul(X, weights['w1']), biases['b1']) #argument for the activation function\n",
    "a1 = tf.nn.sigmoid(z1) #the activation function \n",
    "\n",
    "# hidden layer 2\n",
    "z2 = tf.add(tf.matmul(a1, weights['w2']), biases['b2']) #argument for the activation function\n",
    "a2 = tf.nn.sigmoid(z2) #the activation function \n",
    "\n",
    "#output layer, the output layer need to act on a1\n",
    "# we are not immediatly producing any probabilities here the reason for that is the loss function that we have \n",
    "# beacuse it will get calculated internally \n",
    "\n",
    "# if you want the probability you can do this \n",
    "# yhat = tf.nn.softmax(logits) #will give us the class probalility since we have one class \n",
    "# but since we are doing linear regression we just want the logits\n",
    "\n",
    "logits = tf.add(tf.matmul(a2,weights['w_out']), biases['b_out']) #operates on previous layer output \n",
    "\n",
    "\n",
    "# Back propagation  \n",
    "\n",
    "# define the loss function \n",
    "#RMSE\n",
    "loss = tf.sqrt(tf.reduce_mean(tf.square(Y-logits)))\n",
    "\n",
    "# define the optimizer \n",
    "#optimizer = tf.GradientDesentOptimizer(learning_rate).minimize(loss)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# create session and init the variables \n",
    "session = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "#train over 300 epochs \n",
    "for epoch in range(1, n_epochs+1):\n",
    "    #train on one batch at a time \n",
    "    for i in range(0, len(XTrain), batch_size):\n",
    "        session.run(optimizer, feed_dict={X:XTrain[i:i+batch_size],\n",
    "                                          Y:YTrain[i:i+batch_size]})\n",
    "    #compute training loss for printing progress \n",
    "    if(epoch%10 == 0) | (epoch == 1):\n",
    "        loss_tr = session.run(loss,feed_dict={X:XTrain,Y:YTrain})\n",
    "        print('Epoch {}, loss:{:.3f}'.format(epoch,loss_tr,3))\n",
    "\n",
    "#compute the loss for test data \n",
    "loss_test = session.run(loss,feed_dict={X:XTest,Y:YTest})\n",
    "print(10*'-')\n",
    "print('Test Loss: {:.3f}'.format(np.round(loss_test,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.96254122169644"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(YTest).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
